{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b8a6119",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..data/cleaned_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m DATA_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..data/cleaned_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# assumes your cleaned file is here\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# In[2]: Load cleaned data\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Defensive checks â€” your cleaner should already guarantee these, but just in case:\u001b[39;00m\n\u001b[0;32m     17\u001b[0m required_cols \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvoiceNo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvoiceDate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomerID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStockCode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuantity\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnitPrice\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
      "File \u001b[1;32m~\\.conda\\envs\\ml-env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\ml-env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\.conda\\envs\\ml-env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\ml-env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\.conda\\envs\\ml-env\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..data/cleaned_data.csv'"
     ]
    }
   ],
   "source": [
    "# In[1]: Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Display options for comfort\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "\n",
    "DATA_PATH = \"data/cleaned_data.csv\"  # assumes your cleaned file is here\n",
    "\n",
    "# In[2]: Load cleaned data\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Defensive checks â€” your cleaner should already guarantee these, but just in case:\n",
    "required_cols = {\"InvoiceNo\", \"InvoiceDate\", \"CustomerID\", \"StockCode\", \"Description\", \"Quantity\", \"UnitPrice\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Dataset missing required columns: {missing}\")\n",
    "\n",
    "# Ensure types\n",
    "df[\"CustomerID\"] = df[\"CustomerID\"].astype(str).str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "df[\"StockCode\"] = df[\"StockCode\"].astype(str)\n",
    "df[\"InvoiceDate\"] = pd.to_datetime(df[\"InvoiceDate\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"InvoiceDate\"])\n",
    "\n",
    "# If TotalPrice is missing, compute it\n",
    "if \"TotalPrice\" not in df.columns:\n",
    "    df[\"TotalPrice\"] = df[\"Quantity\"] * df[\"UnitPrice\"]\n",
    "\n",
    "print(df.head(3))\n",
    "print(f\"\\nRows: {len(df):,}  |  Customers: {df['CustomerID'].nunique():,}  |  Items: {df['StockCode'].nunique():,}\")\n",
    "\n",
    "# In[3]: Build userâ€“item interaction matrix\n",
    "# Signal choice: total quantity purchased per item\n",
    "user_item_matrix = df.pivot_table(\n",
    "    index=\"CustomerID\",\n",
    "    columns=\"StockCode\",\n",
    "    values=\"Quantity\",\n",
    "    aggfunc=\"sum\",\n",
    "    fill_value=0,\n",
    ").astype(float)\n",
    "\n",
    "user_item_matrix.shape\n",
    "\n",
    "# In[4]: Compute userâ€“user cosine similarity\n",
    "# Note: cosine_similarity expects a 2D array; we pass the dense values of the matrix.\n",
    "similarity = cosine_similarity(user_item_matrix.values)\n",
    "similarity_df = pd.DataFrame(\n",
    "    similarity, index=user_item_matrix.index, columns=user_item_matrix.index\n",
    ")\n",
    "\n",
    "# Sanity check: similarity is symmetric, diagonal ~ 1.0\n",
    "(similarity_df.values.diagonal()[:5], similarity_df.shape)\n",
    "\n",
    "# In[5]: Helper â€” get top similar users for a given user\n",
    "def top_similar_users(user_id: str, k: int = 10) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Return top-k most similar users (excluding the user).\n",
    "    \"\"\"\n",
    "    uid = str(user_id).replace(\".0\", \"\")\n",
    "    if uid not in similarity_df.index:\n",
    "        return pd.Series(dtype=float, name=\"similarity\")\n",
    "\n",
    "    sims = similarity_df.loc[uid].drop(uid, errors=\"ignore\")\n",
    "    return sims.sort_values(ascending=False).head(k)\n",
    "\n",
    "# In[6]: Recommender â€” user-based CF with weighted scores\n",
    "def recommend_for(user_id: str, top_n: int = 10, min_sim_users: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Recommend items for a user using userâ€“user CF (cosine).\n",
    "    Steps:\n",
    "      1) Get similarity vector for the user (aligned to UIM).\n",
    "      2) Weighted score = UIM^T dot sim_vec / sum(sim_vec)\n",
    "      3) Drop items already purchased by the user.\n",
    "      4) Attach descriptions and return top-N.\n",
    "    Returns either a message or a DF with [StockCode, Description, Estimated Score].\n",
    "    \"\"\"\n",
    "    uid = str(user_id).replace(\".0\", \"\")\n",
    "    if uid not in user_item_matrix.index:\n",
    "        return pd.DataFrame({\"Message\": [f\"Customer {uid} not found.\"]})\n",
    "\n",
    "    # Similarity vector aligned to UIM index\n",
    "    sim_vec = similarity_df.loc[uid].reindex(user_item_matrix.index).fillna(0.0)\n",
    "\n",
    "    # Remove self-similarity\n",
    "    sim_vec = sim_vec.drop(uid, errors=\"ignore\")\n",
    "\n",
    "    # If there are no similar users, early exit\n",
    "    # Optionally, you can filter to only strictly positive similarities\n",
    "    positive_sims = sim_vec[sim_vec > 0]\n",
    "    if len(positive_sims) < min_sim_users or positive_sims.sum() == 0:\n",
    "        return pd.DataFrame({\"Message\": [f\"No similar users found for {uid}.\"]})\n",
    "\n",
    "    # Weighted scores across items: StockCode-indexed Series\n",
    "    weighted_scores = user_item_matrix.T.dot(positive_sims) / positive_sims.sum()\n",
    "\n",
    "    # Remove items the user already bought\n",
    "    already = user_item_matrix.loc[uid]\n",
    "    purchased_codes = already[already > 0].index\n",
    "    weighted_scores = weighted_scores.drop(index=purchased_codes, errors=\"ignore\")\n",
    "\n",
    "    if weighted_scores.empty:\n",
    "        return pd.DataFrame({\"Message\": [f\"No unseen items to recommend for {uid}.\"]})\n",
    "\n",
    "    # Attach item descriptions\n",
    "    meta = (\n",
    "        df[[\"StockCode\", \"Description\"]]\n",
    "        .drop_duplicates(subset=[\"StockCode\"])\n",
    "        .set_index(\"StockCode\")\n",
    "    )\n",
    "\n",
    "    recs = (\n",
    "        pd.DataFrame(\n",
    "            {\"StockCode\": weighted_scores.index, \"Estimated Score\": weighted_scores.values}\n",
    "        )\n",
    "        .join(meta, on=\"StockCode\")\n",
    "        .sort_values(\"Estimated Score\", ascending=False)\n",
    "        .head(top_n)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Safety: ensure Description column exists\n",
    "    if \"Description\" not in recs.columns:\n",
    "        recs[\"Description\"] = \"N/A\"\n",
    "\n",
    "    return recs[[\"StockCode\", \"Description\", \"Estimated Score\"]]\n",
    "\n",
    "# In[7]: (Optional) Explainer â€” show which users drove the recommendation\n",
    "def explain_user_influence(user_id: str, k_users: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Show top-k similar users and their similarity scores.\n",
    "    Useful to debug / explain why recommendations were proposed.\n",
    "    \"\"\"\n",
    "    uid = str(user_id).replace(\".0\", \"\")\n",
    "    sims = top_similar_users(uid, k=k_users)\n",
    "    return sims.reset_index().rename(columns={\"index\": \"SimilarUserID\", uid: \"Similarity\"}).rename(columns={0: \"Similarity\"})\n",
    "\n",
    "# In[8]: Try it out â€” change '17850' to any ID in your dataset\n",
    "TEST_USER = \"17850\"\n",
    "\n",
    "print(\"Top similar users:\")\n",
    "display(explain_user_influence(TEST_USER, k_users=10))\n",
    "\n",
    "print(\"\\nTop recommendations:\")\n",
    "display(recommend_for(TEST_USER, top_n=10))\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# In[9]: (Optional) Save artifacts for inspection or downstream use\n",
    "# These can be big â€” use compressed formats where possible.\n",
    "\n",
    "# Save userâ€“item matrix\n",
    "user_item_matrix.to_csv(\"artifacts/user_item_matrix.csv.gz\", compression=\"gzip\")\n",
    "\n",
    "# Save similarity matrix (can be large; Parquet is more compact than CSV)\n",
    "try:\n",
    "    similarity_df.to_parquet(\"artifacts/user_similarity.parquet\")\n",
    "except Exception:\n",
    "    # Fall back to CSV if parquet engine isn't available\n",
    "    similarity_df.to_csv(\"artifacts/user_similarity.csv.gz\", compression=\"gzip\")\n",
    "\n",
    "print(\"âœ… Saved artifacts in artifacts/\")\n",
    "\n",
    "# In[10]: (Optional) Quick health checks\n",
    "assert user_item_matrix.index.equals(similarity_df.index)\n",
    "assert user_item_matrix.index.equals(similarity_df.columns)\n",
    "print(\"âœ… Indices/columns aligned for dot products\")\n",
    "print(\"UIM shape:\", user_item_matrix.shape, \" | Sim shape:\", similarity_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be61a820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1813373b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD Model RMSE:\n",
      "RMSE: 168446.8075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "168446.807512412"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09d80dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "KNN Model RMSE:\n",
      "RMSE: 312.2391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "312.23911795840945"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6b73489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted TotalPrice for user 17850.0 on item 85123A: 168469.60\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-env)",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
